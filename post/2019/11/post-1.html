<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="b7SMD20d4AzMfKHgolym7URGFijAaqJXJp_sSuHThjw"/>
  <meta name="msvalidate.01" content="F155403EF777BAAE0E5ED44F5E93A691" />

  <title>Huan Li's Personal Page | New Information Venue</title>
  <meta name="description" content="Hi everyone, I am happy to announce that my personal page has been upgraded and moved to the domain of AAU-CS. From now on, I will update my status from time...">
  <link rel="shortcut icon" type="image/png" href="/images/icon.png">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://longaspire.github.io/post/2019/11/post-1">
  <link rel="alternate" type="application/rss+xml" title="Huan Li's Personal Page" href="https://longaspire.github.io/feed.xml" />
<link rel='stylesheet' id='open-sans-css'  href='//fonts.googleapis.com/css?family=Open+Sans%3A300italic%2C400italic%2C600italic%2C300%2C400%2C600&#038;subset=latin%2Clatin-ext&#038;ver=4.2.4' type='text/css' media='all' />
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:600italic,600,400,400italic' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-160136605-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-160136605-1');
</script>

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Huan Li's Personal Page</a>


    <nav class="site-nav">

      <a href="#" class="menu-icon menu.open">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

    <div class="trigger"><h1>Navigation</h1>

 		<ul class="menu">

    
    
     <li><a href="/publication/" class="page-link">Publications</a>
    
    </li>
    
    
     <li><a href="/teaching/" class="page-link">Teaching</a>
    
    </li>
    
    
     <li><a href="/projects/" class="page-link">Projects</a>
    
    </li>
    
    
     <li><a href="/post/" class="page-link">Posts</a>
    
    </li>
    
    
    <li><a href="/miscellany/" class="page-link">Miscellany</a>
    <ul class="sub-menu">
    
    <li><a href="/miscellany/family/">Family Life</a></li>
    
    <li><a href="/miscellany/previous_work/">Previous Work</a></li>
    
    <li><a href="/miscellany/name/">HUAN LI in Chinese</a></li>
    
    </ul>
    
    </li>
    
</ul>


     </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">New Information Venue</h1>
    <p class="post-meta">Posted on November 10, 2019   

  in
  
  <a href="/categories/#personal information" title="personal information">personal information</a>
  


</p>
  </header>

  <article class="post-content">
    <p>Hi everyone, I am happy to announce that my personal page has been upgraded and moved to the domain of AAU-CS. From now on, I will update my status from time to time. Hope I can share more good news and interesting things with you.</p>

<!-- ### Introduction

Recurrent Neural Networks and their variations are very likely to overfit the training data. This is due to the large network formed by unfolding each cell of the RNN, and *relatively* small number of parameters (since they are shared over each time step) and training data. Thus, the perplexities obtained on the test data are often quite larger than expected. Several attempts have been made to minimize this problem using varied **regularization** techniques. This paper tackles this issue by proposing a model that combines several of such existing methods.

*Merity et al*'s model is a modification of the standard **LSTM** in which *DropConnect* is applied to the hidden weights in the *recurrent* connections of the LSTM for regularization. The dropout mask for each weight is preserved and the same mask is used across all time steps, thereby adding negligible computation overhead. Apart from this, several other techniques have been incorporated :
* **Variational dropout** : The same dropout mask is used for a particular recurrent connection in both the forward and backward pass for all time steps. Each input of a mini-batch has a separate dropout mask, which ensures that the regularizing effect due to it isn't identical across different inputs.
* **Embedding dropout** : Dropout with dropout probability $$p_e$$ is applied to word embedding vectors, which results in new word vectors which are identically zero for the dropped words. The remaining word vectors are scaled by $$\frac{1}{1-p_e}$$ as compensation.
* **AR and TAR** : AR (Activation Regularization) and TAR (Temporal Activation Regularization) are modifications of $$L_2$$ regularization, wherein the standard technique is applied to dropped *output activations* and dropped *change in output activations* respectively. Mathematically, the additional terms in the cost function $$J$$ are (here $$\alpha$$ and $$\beta$$ are scaling constants and $$\textbf{D}$$ is the dropout mask) :

$$
J_{AR}=\alpha L_2\left(\textbf{D}_l^t\odot h_l^t\right)\\
J_{TAR}=\beta L_2\left(\textbf{D}_l^t\odot\left(h_l^t - h_l^{t-1}\right)\right)
$$

* **Weight tying** : In this method, the parameters for word embeddings and the final output layer are shared.
* **Variable backpropagation steps** : A random number of BPTT steps are taken instead of a fixed number, whose mean is very close to the original fixed value ($$s$$). The BPTT step-size ($$x$$) is drawn from the following distribution (here $$\mathcal{N}$$ is the Gaussian distribution, $$p$$ is a number close to 0.95 and $$\sigma^2$$ is the desired variance) :

$$
x \sim p\cdot \mathcal{N}\left(s,\sigma^2\right) + (1-p)\cdot \mathcal{N}\left(\frac{s}{2},\sigma^2\right)
$$

* **Independent sizes of word embeddings and hidden layer** : The sizes of the hidden layer and word embeddings are kept independent of each other.

The paper also introduces a new optimization algorithm, namely **Non-monotonically Triggered Averaged Stochastic Gradient Descent** or NT-ASGD, which can be programmatically described as follows :

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">NT_ASGD</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="s">"""
    Input parameters :
    f  - objective function
    t  - stopping criterion
    w0 - initial parameters
    n  - non-monotonicity interval
    L  - number of epochs after which finetuning is done
    lr - learning rate

    Returns :
    parameter(s) that minimize `f`
    """</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">T</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">params</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">logs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w0</span>
    <span class="k">while</span> <span class="n">t</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="c1"># `func_grad` computes gradient of `f` at `w`
</span>        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">func_grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">k</span><span class="o">%</span><span class="n">L</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Compute model's perplexity for current parameters
</span>            <span class="n">v</span> <span class="o">=</span> <span class="n">perplexity</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">v</span> <span class="o">&gt;</span> <span class="nb">min</span><span class="p">(</span><span class="n">logs</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="n">n</span><span class="p">:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">T</span> <span class="o">=</span> <span class="n">k</span>
            <span class="n">logs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># Return the average of best `k-T+1` parameters
</span>    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">):])</span><span class="o">/</span><span class="p">(</span><span class="n">k</span><span class="o">-</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>     </code></pre></figure>

They also combined their **AWD-LSTM** (ASGD Weight Dropped LSTM) with a neural cache model to obtain further reduction in perplexities. A *neural cache model* stores previous states in memory, and predicts the output obtained by a *convex combination* of the output using stored states and the AWD-LSTM.

### Network description
*Merity et al*'s model used a 3-layer weight dropped LSTM with dropout probability `0.5` for **PTB corpus** and `0.65` for **WikiText-2**, combined with several of the above regularization techniques. The different hyperparameters (as referred to in the discussion above) are as follows : hidden layer size ($$H$$) = `1150`, embedding size ($$D$$) = `400`, number of epochs = `750`, $$L$$ = `1`, $$n$$ = `5`, learning rate = `30`, Gradients clipped at `0.25`, $$p$$ = `0.95`, $$s$$ = `70`, $$\sigma^2$$ = `5`, $$\alpha$$ = `2`, $$\beta$$ = `1`, dropout probabilities for input, hidden outputs, final output and embeddings as `0.4`, `0.3`, `0.4` and `0.1` respectively.

Word embedding weights were initialized from $$\mathcal{U}\left[-0.1,0.1\right]$$ and all other hidden weights from $$\mathcal{U}\left[-\frac{1}{\sqrt{1150}},\frac{1}{\sqrt{1150}}\right]$$. Mini-batch size of `40` was used for PTB and `80` for WT-2.

### Result highlights
* 3-layer AWD-LSTM with weight tying attained 57.3 PPL on PTB
* 3-layer AWD-LSTM with weight tying and a continuous cache pointer attained 52.8 PPL on PTB -->

<!-- Write your post content here in normal `markdown`. An example post is shown below for reference.

### Introduction
Regularization is an important step during the training of neural networks. It helps to generalize the model by reducing the possibility of overfitting the training data. There are several types of regularization techniques, with L2 , L1, elastic-net (linear combination of L2 and L1 regularization), dropout and drop-connect being the major ones. While L2, L1 and elastic-net regularization techniques work by constraining the trainable parameters (or *weights*) from attaining large values (so that no drastic changes in output are observed for slight changes in the input; or in other words, they prefer *diffused* weights rather than *peaked* ones), **dropout** and drop-connect work by averaging the task over a dynamically and randomly generated large *ensemble* of networks. These networks are obtained by randomly disconnecting neurons (*dropout*) or weights (*drop-connect*) from the original network so as to obtain a subnetwork on which the training process is carried out (although for $$\approx1$$ epoch for each, since the chance of the same subnetwork being generated again is very rare).

Application of dropout to feedforward neural networks gives promising results. RNNs are thought of as individual *cells* that are *unfolded* over several time-steps, with the input at each time-step being a token of the sequence. When dropout is used for regularizing such a network, the 'disturbance' it generates at each time step propagates over a long interval, thereby decreasing the network's ability to represent long range dependencies. Thus, applying dropout in the standard manner to RNNs fails to give any improvement. It is here where *Zaremba et al*'s research comes into the picture.
### Network description
The network architecture that *Zaremba et al* proposed is quite simple and intuitive. In case of deep RNNs (i.e. RNNs spanning over several layers ($$h$$) where output of $$h_{l-1}^{t}$$ is used as the input for $$h_l^t$$), all connections between the cells in unfolded state can be broadly classified into two categories - *recurrent* and *non-recurrent*. The connections between cells in the same layer i.e. $$h_l^t ~-~ h_l^{t+1}~~\forall t$$ are *recurrent* connections, and those between cells in adjacent layers i.e. $$h_l^t ~-~ h_{l+1}^t~~\forall l$$ are *non-recurrent* connections. *Zaremba et al* suggested that **dropout** should be applied only to non-recurrent connections - thereby preventing problems which arose earlier.

The modified network for LSTM units can be mathematically represented as follows:

Denoting $$T_{m,n}$$ as an affine transformation from $$\mathbb{R}^m\rightarrow\mathbb{R}^n$$ ( i.e. $$T_{m,n}(x)=Wx+b$$ where $$x\in\mathbb{R}^{m\times1}$$, $$W\in\mathbb{R}^{n\times m}$$ and $$b\in\mathbb{R}^{n\times1}$$ and similarly for multiple inputs) and $$\otimes$$ as elementwise multiplication, we have :

$$
f_l^t=\text{sigmoid}\left(T_{N,D}^1\left(\textbf{D}\left(h_{l-1}^t\right) ; h_l^{t-1}\right)\right) \\
i_l^t=\text{sigmoid}\left(T_{N,D}^2\left(\textbf{D}\left(h_{l-1}^t\right) ; h_l^{t-1}\right)\right) \\
o_l^t=\text{sigmoid}\left(T_{N,D}^3\left(\textbf{D}\left(h_{l-1}^t\right) ; h_l^{t-1}\right)\right) \\
u_l^t=\text{tanh}\left(T_{N,D}^4\left(\textbf{D}\left(h_{l-1}^t\right) ; h_l^{t-1}\right)\right) \\
c_l^t=c_l^{t-1}\otimes f_l^t + u_l^t\otimes i_l^t \\
h_l^t = \text{tanh}\left(c_l^t\right)\otimes o_l^t
$$

Here, $$\textbf{D}$$ is the dropout *layer* or operator which sets a subset of its input randomly to zero with dropout probability `p`. This modification can be adopted for any other RNN architecture.

*Zaremba et al* used these architectures for their experiments in which each cell was unrolled for 35 steps. Mini-batch size was 20 for both :

**Medium LSTM** :
Hidden-layer dimension = `650`,
Weights initialized uniformly in `[-0.05,0.05]`,
Dropout probability = `0.5`,
Number of epochs = `39`,
Learning rate = `1` which decays by a factor of `1.2` after 6 epochs,
Gradients clipped at `5`.

**Large LSTM** :
Hidden-layer dimension = `1500`,
Weights initialized uniformly in `[-0.04,0.04]`,
Dropout probability = `0.65`,
Number of epochs = `55`,
Learning rate = `1` which decays by a factor of `1.15` after 14 epochs,
Gradients clipped at `10`.
### Result highlights
* 78.4 PPL on Penn TreeBank dataset using a single **Large LSTM**
* 68.7 PPL on Penn TreeBank dataset using an ensemble of 38 **Large LSTM**s -->

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

<!--     <h2 class="footer-heading">Huan Li's Personal Page</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <!-- <li><strong>Huan Li's Personal Page</strong></li> -->
          <li><span class="icon  icon--github">
            <i class="fas fa-envelope"></i>
          </span><a href="mailto:lihuan AT cs DOT aau DOT dk">lihuan AT cs DOT aau DOT dk</a></li>
          <li><span class="icon  icon--github">
            <i class="fas fa-map-marked-alt"></i>
          </span>Selma Lagerløfs Vej 300, Aalborg University, 9220 Aalborg Øst, Denmark</li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
              <span class="icon  icon--github">
                <!-- <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg> -->
                <i class="fab fa-github-square"></i>
              </span>
            <a href="https://github.com/longaspire">
              <span class="username">longaspire</span>
            </a>
          </li>
          

          <!--  -->

          
          <li>
              <span class="icon  icon--linkedin">
                <!-- <svg viewBox="0 50 512 512" >
                  <path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
                  C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
                  M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
                  c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
                  s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/>
                </svg> -->
                <i class="fab fa-linkedin"></i>
              </span>
            <a href="https://www.linkedin.com/in/lihuancs">
              <span class="username">lihuancs</span>
            </a>
          </li>
          


          
          </svg>
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
         <p class="text">
Remember what should be remembered, and forget what should be forgotten. Alter what is changeable, and accept what is mutable. -- “The Catcher in the Rye”
</p>
<!-- <i>Last update: </i> -->
      </div>
    </div>

  </div>

</footer>

  </body>

</html>